{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Molecular Vae implementation with pytorch\n",
    "\n",
    "※基本的にはhttps://github.com/ShabbirK/chemical_vae_pytorch/blob/main/Chemical%20VAE.ipynbの写経"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pathlib\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 120\n",
    "NLATENT = 196\n",
    "DECODER_HIDDEN_SIZE = 488\n",
    "DECODER_NUM_LAYERS = 3\n",
    "\n",
    "#adopted from https://github.com/aspuru-guzik-group/chemical_vae/tree/main/models/zinc_properties\n",
    "\n",
    "DATA_DIR = './data'\n",
    "DATA_FILE_NAME = '250k_rndm_zinc_drugs_clean_3.csv'\n",
    "CHAR_FILE_NAME = 'zinc.json'\n",
    "\n",
    "#adopted from https://github.com/ShabbirK/chemical_vae_pytorch/blob/main/data/test_idx.npy\n",
    "TEST_IDX_FILE_NAME = 'test_idx.npy'\n",
    "\n",
    "ALL_LETTERS = yaml.safe_load(open(pathlib.Path(DATA_DIR, CHAR_FILE_NAME))) + ['SOS']\n",
    "N_LETTERS = len(ALL_LETTERS) \n",
    "\n",
    "MODELS_DIR = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['7',\n",
       "  '6',\n",
       "  'o',\n",
       "  ']',\n",
       "  '3',\n",
       "  's',\n",
       "  '(',\n",
       "  '-',\n",
       "  'S',\n",
       "  '/',\n",
       "  'B',\n",
       "  '4',\n",
       "  '[',\n",
       "  ')',\n",
       "  '#',\n",
       "  'I',\n",
       "  'l',\n",
       "  'O',\n",
       "  'H',\n",
       "  'c',\n",
       "  '1',\n",
       "  '@',\n",
       "  '=',\n",
       "  'n',\n",
       "  'P',\n",
       "  '8',\n",
       "  'C',\n",
       "  '2',\n",
       "  'F',\n",
       "  '5',\n",
       "  'r',\n",
       "  'N',\n",
       "  '+',\n",
       "  '\\\\',\n",
       "  ' ',\n",
       "  'SOS'],\n",
       " 36)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_LETTERS, N_LETTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEUtils:\n",
    "    \"\"\"\n",
    "    This purpose of this class is \n",
    "    to help with various aspects of data processing\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir=DATA_DIR,\n",
    "                 data_file_name=DATA_FILE_NAME,\n",
    "                 test_idx_file_name=TEST_IDX_FILE_NAME,\n",
    "                 max_len=MAX_LEN,\n",
    "                 all_letters=ALL_LETTERS):\n",
    "        \n",
    "        self.data_dir = pathlib.Path(data_dir)\n",
    "        self.data_file = self.data_dir / pathlib.Path(data_file_name)\n",
    "        self.test_file = self.data_dir / pathlib.Path(test_idx_file_name)\n",
    "\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.all_letters = all_letters #SMILES文字列に含まれるすべての文字\n",
    "        self.n_letters = len(all_letters)\n",
    "        self.letters_to_indices_dict = dict((l, i) for i, l in enumerate(all_letters)) #文字→番号\n",
    "        self.indices_to_letters_dict = dict((i, l) for i, l in enumerate(all_letters)) #番号→文字\n",
    "\n",
    "    def get_data_df(self):\n",
    "        df = pd.read_csv(self.data_file) #データの読み込み\n",
    "        df = df[df.smiles.str.len() <= self.max_len].reset_index(drop=True) #max_lenより長いSMILESの削除\n",
    "\n",
    "        #preprocess input smiles to remove the newline character and add padding\n",
    "        df.loc[:, 'smiles'] = df.loc[:, 'smiles'].str.strip().str.pad(width=self.max_len, side='right', fillchar=\" \")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_input_tensor(self, smile):\n",
    "        tensor = torch.zeros(1, len(smile), self.n_letters) #batch_size * seq_length * num_features\n",
    "        for i, letter in enumerate(smile):\n",
    "            tensor[0][i][self.letters_to_indices_dict[letter]] = 1 #one hot vector\n",
    "        return tensor\n",
    "\n",
    "    def get_target_tensor(self, smile):\n",
    "        letter_indexes = [self.letters_to_indices_dict[l] for l in smile]\n",
    "        # letter_indexes.append(self.n_letters-1) #EOS\n",
    "        return torch.LongTensor(letter_indexes)\n",
    "    \n",
    "    def get_train_valid_test_splits(self, reg_col, valid_pct=1):\n",
    "        df = self.get_data_df()[['smiles', reg_col]]\n",
    "        df = df.rename(columns={reg_col: 'reg_col'})\n",
    "\n",
    "        test_idx = np.load(self.test_file)\n",
    "        non_test_idx = np.array(df[~df.index.isin(test_idx)].index)\n",
    "        train_idx, valid_idx = train_test_split(non_test_idx, test_size=valid_pct,\n",
    "                                                random_state=42, shuffle=True)\n",
    "        \n",
    "        assert len(df)==len(test_idx) + len(train_idx) + len(valid_idx)\n",
    "\n",
    "        return df, train_idx, valid_idx, test_idx\n",
    "    \n",
    "    def get_dl(self, df, idx, bs, shuffle=False):\n",
    "        df = df.iloc[idx]\n",
    "\n",
    "        input_tensors = torch.zeros(len(df), self.max_len, self.n_letters)\n",
    "        target_tensors = torch.zeros(len(df), self.max_len, dtype=torch.long)\n",
    "\n",
    "        for i, smile in enumerate(tqdm(df.smiles)):\n",
    "            input_tensors[i] = self.get_input_tensor(smile)\n",
    "            target_tensors[i] = self.get_target_tensor(smile)\n",
    "\n",
    "        input_tensors = input_tensors\n",
    "        target_tensors = target_tensors\n",
    "\n",
    "        property_values = torch.tensor(df.reg_col.to_numpy()).type(torch.float32)\n",
    "\n",
    "        ds = TensorDataset(input_tensors, target_tensors, property_values)\n",
    "        dl = DataLoader(ds, shuffle=shuffle, batch_size=bs)\n",
    "\n",
    "        return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_utils = VAEUtils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    \"\"\"\n",
    "    This class simplifies layers from custom functions\n",
    "    \"\"\"\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_letters, nlatent, decoder_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_letters = n_letters\n",
    "        self.nlatent = nlatent\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            Lambda(lambda x: x.permute(0, 2, 1)), # the features are in the channels dimension\n",
    "            nn.Conv1d(in_channels=n_letters, out_channels=9, kernel_size=9),\n",
    "            # nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(9), # the authors of the paper did batch normalization\n",
    "            nn.Conv1d(in_channels=9, out_channels=9, kernel_size=9),\n",
    "            # nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(9), # the authors of the paper did batch normalization\n",
    "            nn.Conv1d(in_channels=9, out_channels=11, kernel_size=10),\n",
    "            # nn.Tanh(), # the authors of the paper used tanh\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(11), # the authors of the paper did batch normalization\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.mean = nn.Linear(1045, nlatent)\n",
    "        self.log_var = nn.Linear(1045, nlatent)\n",
    "        self.dec_init_hidden = nn.Linear(nlatent, self.decoder_hidden_size)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        sample = mean + (eps * std)\n",
    "        return sample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean = self.mean(x)\n",
    "        log_var = self.log_var(x)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        dec_init_hidden = self.dec_init_hidden(z)\n",
    "\n",
    "        return z,mean,log_var,dec_init_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_decoder_input(input_tensors):\n",
    "    \"\"\"\n",
    "    Adjust for SOS and make batch the second demension\n",
    "    \"\"\"\n",
    "    sos_tensor = torch.zeros(1, N_LETTERS)\n",
    "    sos_tensor[0][vae_utils.letters_to_indices_dict['SOS']]\n",
    "    new_tensor = torch.zeros(input_tensors.shape[0], MAX_LEN, N_LETTERS)\n",
    "    new_tensor[:][0] = sos_tensor\n",
    "    new_tensor[:, 1:MAX_LEN, :] = input_tensors[:, 0:MAX_LEN-1, :]\n",
    "    new_tensor = new_tensor.permute(1, 0, 2).to(input_tensors.device)\n",
    "\n",
    "    return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.preprocess = Lambda(preprocess_decoder_input)\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers)\n",
    "        self.out = nn.Linear(hidden_size, input_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.rnn(self.preprocess(input),)\n",
    "        output = output.permute(1,0,2)\n",
    "        output = self.softmax(self.out(output))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyPredictor(nn.Module):\n",
    "    def __init__(self, nlatent):\n",
    "        super().__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(nlatent, 1000),\n",
    "            # nn.Tanh() #the authors of the paper used tanh\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.2),\n",
    "            nn.Linear(1000,1000),\n",
    "            # nn.Tanh() #the authors of the paper used tanh\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.2),\n",
    "            nn.Linear(1000,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Helper Classes & Functions for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, reg_col):\n",
    "        self.reg_col = reg_col\n",
    "        models_dir = pathlib.Path(MODELS_DIR)\n",
    "        models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        #encoderとdecoderのモデルを保存する\n",
    "        self.encoder_file = models_dir / pathlib.Path(f'{reg_col}_encoder.pth')\n",
    "        self.decoder_file = models_dir / pathlib.Path(f'{reg_col}_decoder.pth')\n",
    "        self.property_predictor_file = models_dir / pathlib.Path(f'{reg_col}_property_predictor.pth')\n",
    "\n",
    "        #lossを記録する\n",
    "        self.train_losses_file = models_dir / pathlib.Path(f'{reg_col}_train_losses.csv')\n",
    "        self.val_losses_file = models_dir / pathlib.Path(f'{reg_col}_valid_losses.csv')\n",
    "\n",
    "        self.loss_columns = ['total_loss', 'reconstruction_loss', 'kl_divergence', 'regression_loss']\n",
    "        self.train_losses_df = pd.DataFrame(columns=self.loss_columns)\n",
    "        self.val_losses_df = pd.DataFrame(columns=self.loss_columns)\n",
    "\n",
    "    #initialize  networks\n",
    "    def initialize_networks(self):\n",
    "        self.encoder = Encoder(vae_utils.n_letters, NLATENT, DECODER_HIDDEN_SIZE)\n",
    "        self.decoder = Decoder(vae_utils.n_letters, DECODER_HIDDEN_SIZE, DECODER_NUM_LAYERS)\n",
    "        self.property_predictor = PropertyPredictor(NLATENT)\n",
    "\n",
    "    #時間を測る\n",
    "    def __time_since(self, since):\n",
    "        now = time()\n",
    "        s = now - since\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m*60\n",
    "        return f'{m}m {s:.0f}s'\n",
    "    \n",
    "    #for calculate kl divergence\n",
    "    def kl_anneal_function(self, epoch, anneal_start, k=1):\n",
    "        return 1/(1+np.exp(-k*(epoch - anneal_start)))\n",
    "    \n",
    "    def get_losses(self, epoch, device, anneal_start, input_tensors, target_tensors, property_values):\n",
    "        \n",
    "        # loss function \n",
    "        reconstruction_loss_func = nn.NLLLoss()\n",
    "        reg_loss_func = nn.MSELoss()\n",
    "\n",
    "        input_tensors = input_tensors.to(device)\n",
    "        target_tensors = target_tensors.to(device)\n",
    "        property_values = property_values.to(device)\n",
    "\n",
    "        # encode input tensor\n",
    "        z, mean, log_var, dec_init_hidden = self.encoder(input_tensors)\n",
    "\n",
    "        # decode input tensor\n",
    "        output, hidden = self.decoder(input_tensors,\n",
    "                                    dec_init_hidden.unsqueeze(0).repeat(DECODER_NUM_LAYERS, 1, 1))\n",
    "        \n",
    "        # property predict\n",
    "        reg_pred = self.property_predictor(mean)\n",
    "\n",
    "        kl_divergence = -0.5 * torch.sum(1+log_var-mean.pow(2)-log_var.exp())\n",
    "        kl_weight = self.kl_anneal_function(epoch, anneal_start)\n",
    "\n",
    "        reg_loss = reg_loss_func(reg_pred.flatten(), property_values)\n",
    "        reg_loss = reg_loss.type(torch.float32)\n",
    "\n",
    "        reconstruction_loss = reconstruction_loss_func(output.permute(0,2,1), target_tensors)\n",
    "\n",
    "        loss = reconstruction_loss + kl_divergence*kl_weight + reg_loss\n",
    "\n",
    "        return loss, reconstruction_loss, kl_divergence, reg_loss\n",
    "\n",
    "    # process of dataloader\n",
    "    def process_dl(self, epoch, anneal_start, device, dl, train,\n",
    "                   enc_opt=None, dec_opt=None, pp_opt=None):\n",
    "        num_loaders = len(dl)\n",
    "        loader_loss = 0\n",
    "        recon_loss_epoch = 0\n",
    "        kld_epoch = 0\n",
    "        reg_loss_epoch = 0\n",
    "\n",
    "        for input_tensors, target_tensors, property_values in dl:\n",
    "            loss, reconstruction_loss, kl_divergence, reg_loss = \\\n",
    "                self.get_losses(epoch, device, anneal_start, input_tensors, target_tensors, property_values)\n",
    "            \n",
    "            if train:\n",
    "                enc_opt.zero_grad()\n",
    "                dec_opt.zero_grad()\n",
    "                pp_opt.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                enc_opt.step()\n",
    "                dec_opt.step()\n",
    "                pp_opt.step()\n",
    "\n",
    "            loader_loss += loss.item()\n",
    "            recon_loss_epoch += reconstruction_loss.item()\n",
    "            kld_epoch += kl_divergence.item()\n",
    "            reg_loss_epoch += reg_loss.item()\n",
    "\n",
    "        loader_loss /= num_loaders\n",
    "        recon_loss_epoch /= num_loaders\n",
    "        kld_epoch /= num_loaders\n",
    "        reg_loss_epoch /= num_loaders\n",
    "\n",
    "        return (loader_loss, recon_loss_epoch, kld_epoch, reg_loss_epoch)\n",
    "    \n",
    "    def save_parameters_and_losses(self,train_losses:list, val_losses:list):\n",
    "\n",
    "        # save the models\n",
    "        torch.save(self.encoder.state_dict(), self.encoder_file)\n",
    "        torch.save(self.decoder.state_dict(), self.decoder_file)\n",
    "        torch.save(self.property_predictor.state_dict(), self.property_predictor_file)\n",
    "\n",
    "        # save train losses\n",
    "        temp_df = pd.DataFrame(data=train_losses, columns=self.loss_columns)\n",
    "        self.train_losses_df = self.train_losses_df.append(temp_df, ignore_index=True)\n",
    "        self.train_losses_df.to_csv(self.train_losses_file, index=False)\n",
    "\n",
    "        #save val losses\n",
    "        temp_df = pd.DataFrame(data=val_losses, columns=self.loss_columns)\n",
    "        self.val_losses_df = self.val_losses_df.append(temp_df, ignore_index=True)\n",
    "        self.val_losses_df.to_csv(self.val_losses_file, index=False)\n",
    "\n",
    "    def load_parameters_and_losses(self):\n",
    "\n",
    "        #load the models\n",
    "        self.encoder.load_state_dict(torch.load(self.decoder_file))\n",
    "        self.decoder.load_state_dict(torch.load(self.decoder_file))\n",
    "        self.property_predictor.load_state_dict(torch.load(self.property_predictor_file))\n",
    "\n",
    "        #load the losses\n",
    "        self.train_losses_df = pd.read_csv(self.train_losses_file)\n",
    "        self.val_losses_df = pd.read_csv(self.val_losses_file)\n",
    "\n",
    "    def print_result(self, epoch, start_time, train_loss, val_loss):\n",
    "        print(f\"Epoch: {epoch + 1:3d} | \" + \\\n",
    "                  f\"Train Loss: {train_loss[0]:10.5f} | Val Loss: {val_loss[0]:10.5f} | \" + \\\n",
    "                  f\"Time Taken: {self.__time_since(start_time)}\")\n",
    "        print(f\"Train Recon Loss: {train_loss[1]:10.5f} | Val Recon Loss: {val_loss[1]:10.5f}\")\n",
    "        print(f\"Train KLD: {train_loss[2]:10.5f} | Val KLD: {val_loss[2]:10.5f}\")\n",
    "        print(f\"Train Reg Loss: {train_loss[3]:10.5f} | Val Reg Loss: {val_loss[3]:10.5f}\\n\")\n",
    "\n",
    "    # 本体, 学習時に呼び出す関数\n",
    "    def fit(self, epochs, save_every, anneal_start, lr, train_dl, valid_dl, device, load_previous=False):\n",
    "\n",
    "        # initialize networks\n",
    "        self.initialize_networks()\n",
    "\n",
    "        # put networks on the device\n",
    "        self.encoder = self.encoder.to(device)\n",
    "        self.decoder = self.decoder.to(device)\n",
    "        self.property_predictor = self.property_predictor.to(device)\n",
    "\n",
    "        # 学習済みモデルをloadするとき\n",
    "        if load_previous:\n",
    "            self.load_parameters_and_losses()\n",
    "\n",
    "        # optimizerの初期化\n",
    "        enc_opt = optim.Adam(self.encoder.parameters(), lr=lr)\n",
    "        dec_opt = optim.Adam(self.decoder.parameters(), lr=lr)\n",
    "        pp_opt = optim.Adam(self.property_predictor.parameters(), lr=lr)\n",
    "\n",
    "        # loss listの初期化\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        prev_n_epochs = len(self.train_losses_df)\n",
    "\n",
    "        # 時間を測る\n",
    "        start_time = time()\n",
    "\n",
    "        for epoch in tqdm(range(prev_n_epochs, epochs)):\n",
    "\n",
    "            #学習モード\n",
    "            self.encoder.train()\n",
    "            self.decoder.train()\n",
    "            self.property_predictor.train()\n",
    "\n",
    "            #損失の計算 & 誤差伝播\n",
    "            train_loss = self.process_dl(epoch, anneal_start, device, train_dl, True, enc_opt, dec_opt, pp_opt)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            #評価モード\n",
    "            self.encoder.eval()\n",
    "            self.decoder.eval()\n",
    "            self.property_predictor.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #損失の計算 & 誤差伝播\n",
    "                val_loss = self.process_dl(epoch, anneal_start, device, valid_dl, False)\n",
    "                val_losses.append(val_loss)\n",
    "\n",
    "            # 結果の出力\n",
    "            if epoch % 10 == 9:\n",
    "                self.print_result(self, epoch, start_time, train_loss, val_loss)\n",
    "\n",
    "            # モデルとlossesの保存\n",
    "            if epoch % save_every == save_every - 1:\n",
    "                self.save_parameters_and_losses(train_losses,val_losses)\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "\n",
    "        self.save_parameters_and_losses(train_losses, val_losses)\n",
    "\n",
    "        del self.encoder\n",
    "        del self.decoder\n",
    "        del self.property_predictor\n",
    "\n",
    "    # loss plot\n",
    "    def plot_losses(self):\n",
    "        \n",
    "        train_losses_df = pd.read_csv(self.train_losses_file)\n",
    "        val_losses_df = pd.read_csv(self.val_losses_file)\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 15})\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20,10))\n",
    "        \n",
    "                \n",
    "        ax1.plot(train_losses_df['total_loss'], label='Train')\n",
    "        ax1.plot(val_losses_df['total_loss'], label='Validation')\n",
    "        ax1.set_ylabel('Total Loss')\n",
    "        ax1.legend(loc='upper right')\n",
    "        \n",
    "        ax2.plot(train_losses_df['reconstruction_loss'], label='Train')\n",
    "        ax2.plot(val_losses_df['reconstruction_loss'], label='Validation')\n",
    "        ax2.set_ylabel('Reconstruction Loss')\n",
    "        ax2.legend(loc='upper right')\n",
    "        \n",
    "        ax3.plot(train_losses_df['kl_divergence'], label='Train')\n",
    "        ax3.plot(val_losses_df['kl_divergence'], label='Validation')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('KL Divergence')\n",
    "        ax3.legend(loc='upper right')\n",
    "        \n",
    "        \n",
    "        ax4.plot(train_losses_df['regression_loss'], label='Train')\n",
    "        ax4.plot(val_losses_df['regression_loss'], label='Validation')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Regression Loss')\n",
    "        ax4.legend(loc='upper right')\n",
    "        \n",
    "        plt.savefig(pathlib.Path(MODELS_DIR, f\"{self.reg_col}_losses.png\"))\n",
    "        plt.show()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(df, train_idx, valid_idx, max_samples, bs):\n",
    "    start_time = time()\n",
    "    train_dl = vae_utils.get_dl(df, train_idx[:max_samples], bs, shuffle=True)\n",
    "    val_dl = vae_utils.get_dl(df, valid_idx[:max_samples],bs)\n",
    "    print(f'Time taken to get dataloaders: {time() - start_time:.2f}s')\n",
    "\n",
    "    return train_dl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_mae(reg_col, df, test_idx, bs):\n",
    "    encoder = Encoder(vae_utils.n_letters, NLATENT, DECODER_HIDDEN_SIZE)\n",
    "    property_predictor = PropertyPredictor(NLATENT)\n",
    "\n",
    "    encoder.load_state_dict(torch.load(pathlib.Path(MODELS_DIR, f'{reg_col}_encoder.pth')))\n",
    "    property_predictor.load_state_dict(torch.load(pathlib.Path(MODELS_DIR, f'{reg_col}_property_predictor.pth')))\n",
    "\n",
    "    dl = vae_utils.get_dl(df,test_idx, bs)\n",
    "\n",
    "    encoder.eval()\n",
    "    property_predictor.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        abs_errors = []\n",
    "        for input_tensors, target_tensors, property_values in dl:\n",
    "            z, mean, log_var, dec_init_hidden = encoder(input_tensors)\n",
    "            reg_pred = property_predictor(mean)\n",
    "            abs_error = torch.abs(property_values - reg_pred.flatten())\n",
    "            abs_errors.append(abs_error)\n",
    "        \n",
    "        abs_errors = torch.cat(abs_errors)\n",
    "        mae_error = abs_errors.mean()\n",
    "\n",
    "    return mae_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pct = .1\n",
    "bs = 128\n",
    "max_samples = 250000\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "epochs = 120\n",
    "save_every = 2\n",
    "anneal_start = 29\n",
    "lr = 0.0005\n",
    "load_previous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Network for logP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_col = 'logP'\n",
    "df, train_idx, valid_idx, test_idx = vae_utils.get_train_valid_test_splits(reg_col, valid_pct)\n",
    "trainer = Trainer(reg_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224018/224018 [02:49<00:00, 1322.76it/s]\n",
      "100%|██████████| 24891/24891 [00:18<00:00, 1328.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to get dataloaders: 188.35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [02:42<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lisa1\\OneDrive\\デスクトップ\\趣味開発用\\molcular-vae-pytorch\\mol-vae.ipynb Cell 24\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_dl, valid_dl \u001b[39m=\u001b[39m get_dataloaders(df, train_idx, valid_idx, max_samples, bs)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(epochs, save_every, anneal_start, lr, train_dl, valid_dl, device, load_previous\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\lisa1\\OneDrive\\デスクトップ\\趣味開発用\\molcular-vae-pytorch\\mol-vae.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproperty_predictor\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m \u001b[39m#損失の計算 & 誤差伝播\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_dl(epoch, anneal_start, device, train_dl, \u001b[39mTrue\u001b[39;49;00m, enc_opt, dec_opt, pp_opt)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m \u001b[39m#評価モード\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\lisa1\\OneDrive\\デスクトップ\\趣味開発用\\molcular-vae-pytorch\\mol-vae.ipynb Cell 24\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m reg_loss_epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mfor\u001b[39;00m input_tensors, target_tensors, property_values \u001b[39min\u001b[39;00m dl:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     loss, reconstruction_loss, kl_divergence, reg_loss \u001b[39m=\u001b[39m \\\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_losses(epoch, input_tensors, target_tensors, property_values)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39mif\u001b[39;00m train:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m         enc_opt\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;32mc:\\Users\\lisa1\\OneDrive\\デスクトップ\\趣味開発用\\molcular-vae-pytorch\\mol-vae.ipynb Cell 24\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m z, mean, log_var, dec_init_hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(input_tensors)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# decode input tensor\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(input_tensors,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m                             dec_init_hidden\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mrepeat(DECODER_NUM_LAYERS, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# property predict\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m reg_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproperty_predictor(mean)\n",
      "File \u001b[1;32mc:\\Users\\lisa1\\anaconda3\\envs\\molvae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\lisa1\\OneDrive\\デスクトップ\\趣味開発用\\molcular-vae-pytorch\\mol-vae.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, hidden):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(\u001b[39minput\u001b[39;49m),)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lisa1/OneDrive/%E3%83%87%E3%82%B9%E3%82%AF%E3%83%88%E3%83%83%E3%83%97/%E8%B6%A3%E5%91%B3%E9%96%8B%E7%99%BA%E7%94%A8/molcular-vae-pytorch/mol-vae.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(output))\n",
      "File \u001b[1;32mc:\\Users\\lisa1\\anaconda3\\envs\\molvae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\lisa1\\anaconda3\\envs\\molvae\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:998\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    997\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 998\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    999\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m   1000\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m   1002\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_dataloaders(df, train_idx, valid_idx, max_samples, bs)\n",
    "trainer.fit(epochs, save_every, anneal_start, lr, train_dl, valid_dl, device, load_previous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_losses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
